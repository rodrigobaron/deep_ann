{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTE ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define to get all features (id excluded) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_x(df_x):\n",
    "    return df_x[['x0','y0','x1','y1','x2','y2','x3','y3','x4','y4','x5','y5','x6','y6','x7','y7','x8','y8','x9','y9']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define to get all results (id excluded)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(df_y):\n",
    "    return df_y[[\"slope\", \"intercept\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train datasets and process collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = pd.read_csv(\"train_100k.csv\")\n",
    "x_data = preprocess_x(x_data)\n",
    "\n",
    "y_data = pd.read_csv(\"train_100k.truth.csv\")\n",
    "y_data =  preprocess_y(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test datasets and process collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data_test = pd.read_csv(\"train_100k.csv\")\n",
    "x_data_test = preprocess_x(x_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ANN\n",
    "The first, I'm tring to figure out how transfor the two output in one point. So i read again the problem set and decide only to make a simple artificial neural network with two output in a linear activation function (no activation at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Configuration  \n",
    "Some tests:  \n",
    "Epoch: 1000 cost = 0.138191439 cost2 = 47.019507828 4x200  \n",
    "Epoch: 1000 cost = 0.017634519 cost2 = 11.450007486 5x400  \n",
    "Epoch: 1000 cost = 0.008747602 cost2 = 6.681912537 4x500 learning rate 0.0001  \n",
    "\n",
    "So, how more hidden reurons and smaller learning rate, more performance in the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "training_epochs = 5000\n",
    "batch_size = 1000\n",
    "display_step = 100\n",
    "train_data_size = 100000\n",
    "\n",
    "n_hidden = 500 # number of neurons on hidden layer\n",
    "\n",
    "n_input = 20\n",
    "n_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features and response tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors w'll be optimized:  \n",
    "5 hidden layers  \n",
    "1 output layer  \n",
    "5 hidden bias  \n",
    "1 output bias  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden],stddev=0.1)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden, n_hidden],stddev=0.1)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden, n_hidden],stddev=0.1)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden, n_hidden],stddev=0.1)),\n",
    "    'h5': tf.Variable(tf.random_normal([n_hidden, n_hidden],stddev=0.1)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_output],stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'b5': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a fully connected ANN .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.tanh(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.tanh(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.tanh(layer_3)\n",
    "    \n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.tanh(layer_4)\n",
    "    \n",
    "    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']), biases['b5'])\n",
    "    layer_5 = tf.nn.tanh(layer_5)\n",
    "\n",
    "    out_layer = tf.matmul(layer_5, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run feed forward ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = make_network(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slope_cost = tf.reduce_mean(tf.square(y_pred[:,0]-y[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intercept_cost = tf.reduce_mean(tf.abs(y_pred[:,1]-y[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slope_optimizer =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(slope_cost)\n",
    "intercept_optimizer =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(intercept_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create batches to train in more small steps an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 cost = 0.032156546 cost2 = 25.504869118\n",
      "Epoch: 200 cost = 0.019468157 cost2 = 17.121879854\n",
      "Epoch: 300 cost = 0.016755923 cost2 = 12.418868332\n",
      "Epoch: 400 cost = 0.014511940 cost2 = 11.578304768\n",
      "Epoch: 500 cost = 0.022932198 cost2 = 11.408332253\n",
      "Epoch: 600 cost = 0.018589433 cost2 = 8.254057159\n",
      "Epoch: 700 cost = 0.011675939 cost2 = 11.253377357\n",
      "Epoch: 800 cost = 0.012578157 cost2 = 10.440568056\n",
      "Epoch: 900 cost = 0.011732464 cost2 = 9.041368961\n",
      "Epoch: 1000 cost = 0.013583385 cost2 = 7.999374356\n",
      "Epoch: 1100 cost = 0.006592688 cost2 = 6.349081912\n",
      "Epoch: 1200 cost = 0.007857357 cost2 = 6.502352700\n",
      "Epoch: 1300 cost = 0.005654996 cost2 = 5.371522698\n",
      "Epoch: 1400 cost = 0.004334838 cost2 = 5.226565807\n",
      "Epoch: 1500 cost = 0.005200569 cost2 = 5.618376720\n",
      "Epoch: 1600 cost = 0.006270270 cost2 = 7.081418505\n",
      "Epoch: 1700 cost = 0.005161653 cost2 = 5.287339933\n",
      "Epoch: 1800 cost = 0.004054777 cost2 = 4.678557041\n",
      "Epoch: 1900 cost = 0.007042963 cost2 = 5.421184981\n",
      "Epoch: 2000 cost = 0.010520406 cost2 = 5.301299365\n",
      "Epoch: 2100 cost = 0.003580831 cost2 = 3.989310226\n",
      "Epoch: 2200 cost = 0.005877574 cost2 = 3.092553041\n",
      "Epoch: 2300 cost = 0.001501880 cost2 = 2.909424908\n",
      "Epoch: 2400 cost = 0.007935591 cost2 = 4.261391258\n",
      "Epoch: 2500 cost = 0.007654355 cost2 = 3.414490376\n",
      "Epoch: 2600 cost = 0.004923593 cost2 = 2.917642245\n",
      "Epoch: 2700 cost = 0.002168917 cost2 = 2.733939860\n",
      "Epoch: 2800 cost = 0.001865550 cost2 = 2.769375696\n",
      "Epoch: 2900 cost = 0.003778817 cost2 = 3.449384611\n",
      "Epoch: 3000 cost = 0.002543625 cost2 = 3.129689348\n",
      "Epoch: 3100 cost = 0.002166759 cost2 = 2.604199680\n",
      "Epoch: 3200 cost = 0.001841177 cost2 = 2.625728576\n",
      "Epoch: 3300 cost = 0.005386318 cost2 = 2.684453552\n",
      "Epoch: 3400 cost = 0.006066862 cost2 = 2.762970710\n",
      "Epoch: 3500 cost = 0.002205288 cost2 = 2.334938337\n",
      "Epoch: 3600 cost = 0.002162068 cost2 = 2.503151615\n",
      "Epoch: 3700 cost = 0.001976168 cost2 = 2.449244367\n",
      "Epoch: 3800 cost = 0.002980338 cost2 = 2.372636350\n",
      "Epoch: 3900 cost = 0.004274634 cost2 = 2.688194594\n",
      "Epoch: 4000 cost = 0.008762182 cost2 = 2.796896307\n",
      "Epoch: 4100 cost = 0.001255412 cost2 = 2.062843337\n",
      "Epoch: 4200 cost = 0.001981171 cost2 = 1.776167825\n",
      "Epoch: 4300 cost = 0.001715914 cost2 = 2.108067932\n",
      "Epoch: 4400 cost = 0.001459183 cost2 = 2.004871262\n",
      "Epoch: 4500 cost = 0.001369148 cost2 = 1.908648820\n",
      "Epoch: 4600 cost = 0.003404345 cost2 = 1.922473391\n",
      "Epoch: 4700 cost = 0.005881792 cost2 = 2.162117805\n",
      "Epoch: 4800 cost = 0.001798312 cost2 = 1.862763714\n",
      "Epoch: 4900 cost = 0.002454543 cost2 = 1.763024350\n",
      "Epoch: 5000 cost = 0.002368334 cost2 = 1.807959659\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(train_data_size / batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        avg_cost2 = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_data[i * batch_size:min(i * batch_size + batch_size, train_data_size), :], \\\n",
    "                             y_data[i * batch_size:min(i * batch_size + batch_size, train_data_size), :]\n",
    "            _,  slope_c, _, intercept_c = sess.run([slope_optimizer,  slope_cost, intercept_optimizer,  intercept_cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += slope_c / total_batch\n",
    "            avg_cost2 += intercept_c / total_batch\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print('Epoch:', (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'cost2 =', '{:.9f}'.format(avg_cost2))\n",
    "    \n",
    "    # Generate train result \n",
    "    r = sess.run(y_pred, feed_dict = {x: x_data})\n",
    "    r = pd.DataFrame(r, columns=[\"slope\", \"intercept\"])\n",
    "    r.to_csv(\"submission.train_100k.csv\", index=True, index_label='id')\n",
    "    \n",
    "    # Generate test result\n",
    "    r = sess.run(y_pred, feed_dict = {x: x_data_test})\n",
    "    r = pd.DataFrame(r, columns=[\"slope\", \"intercept\"])\n",
    "    r.to_csv(\"submission.test_100k.csv\", index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = MSE  \n",
    "cost2 = MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ python evaluate.py train_100k.truth.csv submission.train_100k.csv  \n",
    "Slope mse: 0.0114102844069  \n",
    "Intercept mae: 2.29707436696  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
